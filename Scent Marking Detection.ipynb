{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directions for use of Python scripts\n",
    "To encourage uptake of the methods described in this study, we have provided all of the Python code necessary to implement detection of scent-marking detection in accelerometer data, and link those detected events to GPS data collected concurrently. Some devices may produce data files that follow a different format, but the code here should be general enough to implement for most devices with some minor adjustment to the code. In this notebook, we describe the sequential steps of the workflow. In steps that require third party software, we provide links to where this software can be downloaded (all are freely available) and will endevour to make sure these links are updated when software is hosted elsewhere. For steps that require use of our Python code, we provide the code itself in the cell that follows each stage description. \n",
    "\n",
    "1. When using the AX3 accelerometer, use OMGui (https://github.com/digitalinteraction/openmovement/wiki/AX3-GUI) to convert the .cwa files in to a resampled CSV file. We down-sampled data to 25 Hz to improve computation speed.\n",
    "    \n",
    "2.\tDo a quick summary plot using the crop_file.py. We used this script to crop the file, as the device continued to record after it was removed from the dogs, producing large files with superfluous data. Use the ‘chunksize’ argument to specify how many rows should be taken from the start of the file. The code in crop_file.py ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "chunksize = 10*10**4 #change here to set number of rows to load from start of file\n",
    "\n",
    "os.chdir('C:\\\\DataFolder\\...')\n",
    "filename = '' #file name in folder here\n",
    "\n",
    "chunk = pd.read_csv(filename, nrows =chunksize)\n",
    "\n",
    "#Convert time column to datetime format    \n",
    "chunk['Time'] = pd.to_datetime(chunk['Time'])\n",
    "\n",
    "##Look at last time in chunk\n",
    "chunk.tail(1)[3]\n",
    "\n",
    "\n",
    "#plot axes to see if whole deployment is present\n",
    "time = chunk.iloc[:,0]\n",
    "x = chunk.iloc[:,1]\n",
    "y = chunk.iloc[:,2]\n",
    "z = chunk.iloc[:,3]\n",
    "\n",
    "#plot just last 100 rows to see if the device is still attached\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(time.tail(100), y.tail(100), color='lightblue', linewidth=3)\n",
    "plt.show()\n",
    "\n",
    "#shows we have the right file size\n",
    "#now write to csv\n",
    "chunk.to_csv('') # pick a filename for output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tBefore we can use the KNN to predict scent-marking events, we need to use an initial period of observation to train the model. In our study, we obtained a training set by observing the dog prior to release. Thus, the accelerometer file must be split in to training and testing periods. The next few steps detail how one may prepare and train the KNN.\n",
    "\n",
    "4.\tWe must synchronize the timestamp of the accelerometer data with the video that details to behavioural observations of the dog. At the beginning of the observation period, we performed conspicuous calibration tilts (the device was rotated 90 degrees for 5 second, and this was repeated 3 times). With this in mind, use subsample.py to plot 10-minute summaries of the data. The function will run through the first 3 days of the file (after that the tag was off the animal and so it's just excess data), showing a 10 minute window in each plot to help you find the calibration tilts mentioned previously. This function needs there to be a folder called 'explore', which will be filled with plots for you to run through sequentially (using Windows’ Photo app for instance). In future, we will implement a more elegant interactive plot for Users to use in Python, but this is currently in development. Once the tilts are identified and their time in the accelerometer file noted, you can trim the start of the file to ca. 5 seconds before the tilts, to make them easy to spot and sync to the video in ELAN. The code for subsample.py ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\DataFolder\\...') #Data location here\n",
    "filename = '' #filename from crop_file.py\n",
    "dog_df = pd.read_csv(filename, usecols = ['Time', 'Accel-X (g)', ' Accel-Y (g)', ' Accel-Z (g)']) #Use only specific columns, this arguement can be changed based on format produced by device\n",
    "dog_df.set_index('Time', inplace=True) #set the index to time column\n",
    "\n",
    "twentyHz = dog_df.iloc[1::2,:] #take every 2nd row to downsample (halves the sample rate)\n",
    "\n",
    "twentyHz.to_csv('dog_dfmine_Twenty.csv') #save a copy of downsampled data\n",
    "\n",
    "##Plot in matplotlib the first 1000 rows\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(twentyHz.head(1000), color='lightblue', linewidth=3) \n",
    "plt.show()\n",
    "\n",
    "#####################################################\n",
    "#Search for calibration tilts\n",
    "# code needs an output folder, called 'explore'\n",
    "path = 'C:\\\\DataFolder\\explore'\n",
    "try:  \n",
    "    os.mkdir(path)\n",
    "except OSError:  \n",
    "    print (\"Creation of the directory %s failed\" % path)\n",
    "else:  \n",
    "    print (\"Successfully created the directory %s \" % path)\n",
    "\t\n",
    "#Here we can use the datetime record in our accl data, and the time the device was started, to produce an 'elapsed seconds' column\n",
    "twentyHz['seconds'] = (pd.to_datetime(twentyHz.index) - pd.to_datetime(pd.DataFrame({'year': [2018],'month': [6],'day': [13], 'hour':[9], 'minute':[36], 'second':[27.525]}))[0]).total_seconds()\n",
    "twentyHz.set_index(twentyHz['seconds'], inplace=True) #change the index to seconds elapsed\n",
    "twentyHz['minutes'] = twentyHz['seconds']/60\n",
    "twentyHz.set_index(twentyHz['minutes'], inplace=True)\n",
    "\n",
    "#For this example, I made plots for a range of 0 to 1440 (1440 minutes is 24 hours)\n",
    "#With that, I found the tilts between 132 and 134 minutes in to the file\n",
    "#Your data may differ\n",
    "\n",
    "for counter in range(0,1440):\n",
    "    start = counter*1\n",
    "    stop = start + 10\n",
    "    fig = twentyHz[(twentyHz['minutes'] > start) & (twentyHz['minutes'] < stop)].iloc[:,0:3].plot(figsize=(20,10)).get_figure()\n",
    "    dir_name = 'C:\\\\Data\\Hopland Dogs\\dog_dfmine\\explore'\n",
    "    base_filename = '{0}{1}'.format('output',counter)\n",
    "    suffix = '.jpg'\n",
    "    fig.savefig(os.path.join(dir_name, base_filename + suffix))\n",
    "    plt.close(fig)\n",
    "    #input(\"Press Enter to continue...\")\n",
    "\n",
    "#Load back in the dog_df 20Hz data\n",
    "twentyHz = pd.read_csv('dog_df_Twenty.csv')\n",
    "twentyHz.set_index('Time', inplace=True)\n",
    "twentyHz['seconds'] = (pd.to_datetime(twentyHz.index) - pd.to_datetime(pd.DataFrame({'year': [2018],'month': [6],'day': [13], 'hour':[9], 'minute':[36], 'second':[27.525]}))[0]).total_seconds()\n",
    "twentyHz['minutes'] = twentyHz['seconds']/60\n",
    "   \n",
    "#To write this one, I used the 'minutes' column, the tilts start ~27 minutes in to the data   \n",
    "start = 27\n",
    "stop = start + 2\n",
    "twentyHz[(twentyHz['minutes'] > start) & (twentyHz['minutes'] < stop)].iloc[:,0:3].plot(figsize=(20,10)) #are tilts visible? \n",
    "\n",
    "cut = twentyHz[(twentyHz['minutes'] > 27)] #cut the file to just before the calibration tilts\n",
    "cut.head(1000).iloc[:,0:3].plot() #check with a quick plot\n",
    "\n",
    "#set seconds to start from 0\n",
    "cut.iloc[0,]\n",
    "cut['seconds'] = cut['seconds']-1620.040000\n",
    "cut['minutes'] = cut['minutes']-27.000667\n",
    "cut.iloc[0:1000,0:3].plot(figsize=(20,10))\n",
    "\n",
    "cut.to_csv('dog_df_Cut.csv') #write to file, for loading in to ELAN\n",
    "print('Finished writing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tLoad the trimmed data in to ELAN, along with the video. Sync the two using the procedure detailed in the video tutorial kindly produced by Cassim Ladha (https://www.youtube.com/watch?v=zofLvUU0Gus). Then run through the video and make annotations. Following this, output annotations as a tab delimited text file to load back in to Python.\n",
    "\n",
    "6.\tUsing the label_accl.py script, load the accelerometer data and the annotation record, combine the two to produce one annotated accelerometer dataset. This will output an accelerometer file with an extra column that contains the class of behaviour being undertaken by the animal at that time. The code for label_accl.py ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is to annotate the accl data using the annotation record from ELAN\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load accl data and set time to start from the first calibration point\n",
    "os.chdir('C:\\\\DataFolder\\...') #Data location here\n",
    "filename = 'dog_df_Cut.csv' #the output from subsample.py\n",
    "twentyHz = pd.read_csv(filename)\n",
    "\n",
    "twentyHz.set_index('seconds', inplace=True)\n",
    "\n",
    "# In this example, ELAN said the offset was 21 seconds, so plot to make sure and then cut\n",
    "start = 20.6\n",
    "stop = start + 40\n",
    "twentyHz[(twentyHz.index > start) & (twentyHz.index < stop)].plot(figsize=(10, 5))\n",
    "\n",
    "# from annotation, we can see the video ends at 31 minutes, 1.653 seconds\n",
    "# that's 1861.653 seconds long\n",
    "\n",
    "# For later use, create csv for the unobserved period that begins immmediately at the end of the video\n",
    "df_unobs = twentyHz[(twentyHz.index > 1861.653)]\n",
    "df_unobs.iloc[::20, :].iloc[:,0:4].plot() #quickly plot every 20th row to get a summary of file\n",
    "\n",
    "\n",
    "#I will pass all data through KNN and then cut the end according to the timestamp\n",
    "df_unobs.to_csv('dog_unobserved.csv') #a file that contains only unobserved data, from free living deployments etc.\n",
    "\n",
    "# cut another file to duration of video for KNN training and validation\n",
    "twentyHz = twentyHz[(twentyHz.index > start) & (twentyHz.index < 1861.653)]\n",
    "# set start time to 0\n",
    "twentyHz.index = twentyHz.index - twentyHz.index[0]\n",
    "\n",
    "# now load in annotations\n",
    "annot = pd.read_table('dog_Annotate.txt', delimiter=\"\\t\", header=None)\n",
    "annot.columns = ['class', 'nans', 'start', 'end', 'duration', 'text'] #the columns typically output from ELAN\n",
    "annot.start.iloc[0] = 0.0\n",
    "\n",
    "# add column to put our annotations\n",
    "import numpy as np\n",
    "\n",
    "twentyHz['class'] = np.nan #fill with NaN for now\n",
    "\n",
    "#The following code takes the behavioural record from annot and uses them to populate the 'class' column in twentyHz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "bins = np.unique(annot[['start', 'end']].values.flatten())  # get time range bins\n",
    "annot['period'] = '['+annot['start'].astype(str) + ', ' + annot['end'].astype(str) + ')'\n",
    "categorized = pd.cut(twentyHz.index, bins, right=False).astype(str) # categorize into bins based on time, right=False means start is inclusive, end is exclusive\n",
    "categorized = pd.Series(categorized).to_frame('labels') # make it a dataframe to get ready for merge below\n",
    "twentyHz['class'] = categorized.merge(annot, left_on='labels', right_on='['+annot['start'].round(decimals=3).astype(str) + ', ' + annot['end'].round(decimals=3).astype(str) + ')', how='left')['class'].fillna('no class').values # merge with processed annot frame to get the correct labels\n",
    "\n",
    "\n",
    "twentyHz.to_csv('dog_annot_accl.csv') #this accelerometer file now has behavioural annotations for use in the KNN\n",
    "\n",
    "print('Script Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tUse the KNN_sm.py script to actually train and test the KNN classifier to the annotated data. Functions to obtain performance metrics (e.g. Accuracy) are included, along with a means to construct confusion matrices to evaluate the KNN’s performance. KNN_sm.py code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('C:\\\\DataFolder\\...') #Data location here\n",
    "\n",
    "#Load in annotated accl data\n",
    "accl_data = pd.read_csv('dog_annot_accl.csv') #from label_accl.py\n",
    "accl_data.set_index('seconds', inplace=True) #set the index\n",
    "\n",
    "#remove all calibration stuff, and data that wasn't labelled at the start of the file\n",
    "accl_data = accl_data[accl_data['class'] != 'calibration']\n",
    "\n",
    "#set x (accl data) and y (classes)\n",
    "x = accl_data.iloc[:,1:4]\n",
    "y = accl_data['class']\n",
    "\n",
    "#Split accl_data to testing and training sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42, shuffle=True)\n",
    "\n",
    "#check training and testing have the same classes in them\n",
    "set(y_train) == set(y_test)\n",
    "\n",
    "## Import the KNN Classifier.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "## Start the KNN instance, setting k to 5 neighbors. \n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "## Fit the model on the training data.\n",
    "knn.fit(X_train, y_train.values.ravel())\n",
    "## See how the model performs on the test data.\n",
    "knn.score(X_test, y_test) #This is the accuracy\n",
    "\n",
    "##Get Array of all predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "##Calculate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "accuracy_score(y_test, y_pred) #Accuracy\n",
    "cohen_kappa_score(y_test, y_pred) #Cohen's Kappa\n",
    "\n",
    "##Plot a confusion matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "##Get confusion matrix    \n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_names = y_test.unique()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##Build classification report\n",
    "from sklearn.metrics import classification_report\n",
    "#target_names = ['class 1', 'class 2', 'class 3', 'class 4']\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "##Join x_test and y_pred for output\n",
    "output = X_test\n",
    "output['class'] = y_pred\n",
    "output.sort_index(inplace = True)\n",
    "output.to_csv('dog_pred_output.csv')\n",
    "\n",
    "#plot output\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for plotting\n",
    "t = np.arange(0.0, 2.0, 0.01)\n",
    "s = 1 + np.sin(2 * np.pi * t)\n",
    "\n",
    "# Let's plot predicted activity and actual activity\n",
    "#Predicted\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(output.index, output['class'], 'o')\n",
    "\n",
    "ax.set(xlabel='time (s)', ylabel='behavior',\n",
    "       title='Predicted Activity - Nate')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "#fig.savefig(\"predicted.png\")\n",
    "\n",
    "#Actual\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(accl_data.index, accl_data['class'], 'o')\n",
    "\n",
    "ax.set(xlabel='time (s)', ylabel='behavior',\n",
    "       title='Actual Activity - Nate')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "#fig.savefig(\"actual.png\")\n",
    "\n",
    "#perform a full prediction\n",
    "accl_data['pred_class'] = knn.predict(accl_data.iloc[:,1:4])\n",
    "accl_data\n",
    "knn.score(accl_data.iloc[:,1:4], accl_data['class']) #92.6%\n",
    "accl_data.to_csv('dog_actual_pred.csv')\n",
    "\n",
    "\n",
    "#Here we predict activity during the unobserved period (from full deployment etc.) \n",
    "#using the KNN that has been trained and validated above\n",
    "dog_unobs = pd.read_csv('dog_unobserved.csv') #from label_accl.py script\n",
    "dog_unobs.set_index('seconds', inplace=True)\n",
    "dog_unobs.head(10)\n",
    "dog_unobs['pred_class'] = knn.predict(dog_unobs.iloc[:,1:4])\n",
    "dog_unobs.head(10)\n",
    "dog_unobs.to_csv('dog_unobs_predicted.csv')\n",
    "\n",
    "#KNN finished, now use smooth_knn.py to prepare predictions for matching with GPS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tUse the smooth_knn.py script to smooth the KNN predictions to 1 Hz so that they can be linked to GPS locations recorded at that time. This script takes the 25 Hz accelerometer data and smooths it to 1 Hz, by taking the modal class for each 25-row group that constitutes a second of accelerometer data. There is also a function in this script to check each class' accuracy and ensure the smoothed data matches the actual classes as recorded by video (after those annotations too has been smoothed). The smooth_knn.py code;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('C:\\\\DataFolder\\...') #Data location here\n",
    "\n",
    "#Load in annotated accl data\n",
    "accl_data = pd.read_csv('dog_actual_pred.csv') #data with actual and predicted classes from behavioural validation period \n",
    "accl_data.set_index('seconds', inplace=True)\n",
    "accl_data['r_secs'] = np.around(accl_data.index, decimals = 0) #define the second epoch across which all sub-second data will be smoothed\n",
    "\n",
    "#produce a downsampled (to 1 Hz) dataset, take the modal class from each second\n",
    "smooth = pd.DataFrame(index = np.around(accl_data.index, decimals = 0).unique())\n",
    "smooth['actual'] = accl_data.groupby('r_secs')['class'].agg(lambda x:x.value_counts().index[0])\n",
    "smooth['pred'] = accl_data.groupby('r_secs')['pred_class'].agg(lambda x:x.value_counts().index[0])\n",
    "\n",
    "\n",
    "# Let's plot predicted activity and actual activity\n",
    "#Predicted\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(smooth.index, smooth['pred'], 'o')\n",
    "\n",
    "ax.set(xlabel='time (s)', ylabel='behavior',\n",
    "       title='Predicted Activity - Dog')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "#fig.savefig(\"predicted.png\")\n",
    "\n",
    "#Actual\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(smooth.index, smooth['actual'], 'o')\n",
    "\n",
    "ax.set(xlabel='time (s)', ylabel='behavior',\n",
    "       title='Actual Activity - Dog')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "#Looks pretty good, let's make sure our scent marks are being detected decently\n",
    "\n",
    "#run quick accuracy check\n",
    "def check(x, y):\n",
    "    if len(x) == len(y):\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        xy = np.where(y == x, 1, 0)\n",
    "    else:\n",
    "        dfx = pd.DataFrame(x)\n",
    "        dfy = pd.DataFrame(y)\n",
    "        df = dfx.merge(dfy, how='outer', on='seconds')\n",
    "        df['cor'] = np.where(df.pred == df.actual, 1, 0)\n",
    "        xy = df['cor']\n",
    "    return np.mean(xy)\n",
    "\n",
    "check(smooth['actual'],smooth['pred']) # 95.5% accuracy on the whole\n",
    "\n",
    "#Can perform behaviour specific accuracy calculations here if you wish\n",
    "#check(smooth['pred'][smooth.pred == 'right'],smooth['actual'][smooth.actual == 'right']) # \n",
    "#check(smooth['pred'][smooth.pred == 'left'],smooth['actual'][smooth.actual == 'left']) # \n",
    "#check(smooth['pred'][smooth.pred == 'scent'],smooth['actual'][smooth.actual == 'scent']) # 85.7% accuracy for left\n",
    "#check(smooth['pred'][smooth.pred == 'lie'],smooth['actual'][smooth.actual == 'lie']) #99%\n",
    "#check(smooth['pred'][smooth.pred == 'stand'],smooth['actual'][smooth.actual == 'stand']) #77%\n",
    "\n",
    "#Check accuracy, seems pretty good, export smooth data ready to be synced to GPS\n",
    "smooth.to_csv('dog_smooth_accl.csv')\n",
    "\n",
    "#Run smoothing over the unobserved period predictions too\n",
    "#Load in annotated accl data\n",
    "dog_unobs = pd.read_csv('dog_unobs_predicted.csv') #file containing unobserved predictions, from KNN_sm.py\n",
    "dog_unobs.set_index('seconds', inplace=True)\n",
    "dog_unobs.head(10)\n",
    "dog_unobs['r_secs'] = np.around(jas_unobs.index, decimals = 0)\n",
    "dog_unobs.head(10)\n",
    "\n",
    "#produce a downsampled (to 1 Hz) dataset, take the modal class from each second\n",
    "smooth_unobs = pd.DataFrame(index = np.around(dog_unobs.index, decimals = 0).unique())\n",
    "smooth_unobs['pred'] = dog_unobs.groupby('r_secs')['pred_class'].agg(lambda x:x.value_counts().index[0])\n",
    "smooth_unobs.head(10)\n",
    "smooth_unobs.to_csv('dog_unobs_smooth_preds.csv') #write out the 1 Hz predictions, ready to be linked to GPS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tUsing the link_gps.py script, merge the predicted classes of the accelerometer data (a product of the KNN) to the Latitude and Longitude values in the GPS data. Once the GPS data has behavioural classes, identify the points where dogs are making left and right scent marks. These points then can be written to file for use in GIS software or R for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('C:\\\\DataFolder\\...') #Data location here\n",
    "\n",
    "#Load in annotated accl data\n",
    "smooth = pd.read_csv('dog_smooth_accl.csv') #smoothed predictions from smooth_knn.py\n",
    "smooth.set_index('seconds', inplace=True)\n",
    "\n",
    "#change seconds in to actual date and time\n",
    "#Note for this dog, 445 seconds on accelerometer time is 2018-06-13 10:11:13.165\n",
    "\n",
    "#Here's some code explaining my method\n",
    "#this line will define a datetime that described actual time at 445 seconds\n",
    "pd.to_datetime(pd.DataFrame({'year': [2018],'month': [6],'day': [13], 'hour':[10], 'minute':[11], 'second':[13.165]}))[0]\n",
    "\n",
    "#make an array that describes the time difference (in datetime) using the index values in seconds\n",
    "pd.to_timedelta(smooth.index, 's')\n",
    "smooth.tail(10)\n",
    "pd.to_timedelta(pd.Series(range(0,1841)), 's')\n",
    "\n",
    "\n",
    "#define a datetime column by adding the timedelta to the origin\n",
    "smooth['datetime'] = pd.to_datetime(pd.DataFrame({'year': [2018],'month': [6],'day': [13], 'hour':[10], 'minute':[11], 'second':[13]}))[0] + pd.to_timedelta(pd.Series(range(0,1842)), 's')\n",
    "\n",
    "#now smooth['datetime'] will describe the actual time, which can be linked to the GPS timestamp\n",
    "\n",
    "#Now do the same for the unobserved data\n",
    "dog_unobs = pd.read_csv('dog_unobs_smooth_preds.csv') #smoothed predictions for the unobserved period, from smooth_knn.py\n",
    "dog_unobs.set_index('seconds', inplace=True)\n",
    "dog_unobs['datetime'] = pd.to_datetime(pd.DataFrame({'year': [2018],'month': [6],'day': [13], 'hour':[10], 'minute':[11], 'second':[13]}))[0] + pd.to_timedelta(dog_unobs.index, 's')\n",
    "\n",
    "#a few lines here to check everything is as it should be\n",
    "dog_unobs.columns\n",
    "dog_unobs['Time']\n",
    "dog_unobs['datetime']\n",
    "#check that they match up\n",
    "smooth.head(10)\n",
    "smooth.tail(10)\n",
    "dog_unobs.head(10) #looks good!\n",
    "\n",
    "#Load in GPS data\n",
    "fields = ['Date', ' Time', ' Latitude', ' Longitude'] #which columns to take from CSV file, many devices produce additional columns, NOTE: the IgotU device introduced a space before some column names i.e. \" Time\"\n",
    "gps = pd.read_csv('Dog_GPS.csv', usecols=fields) #Your file name may differ, define it here\n",
    "gps.columns = ['Date', 'Time', 'Latitude', 'Longitude'] #Remove those spaces from column names\n",
    "gps.set_index(pd.to_datetime(gps.iloc[:,0] + ' ' + gps.iloc[:,1]), inplace=True)\n",
    "\n",
    "#cut GPS data to observed validation period start/end\n",
    "start = smooth.datetime.min()\n",
    "end = smooth.datetime.max()\n",
    "cut_gps = gps[(gps.index >= start) & (gps.index <= end)]\n",
    "\n",
    "#merge labels to gps data\n",
    "smooth_gps = smooth.merge(cut_gps, left_on=smooth.datetime, right_on=cut_gps.index, how='left')\n",
    "smooth_gps.to_csv('dog_gps_labeled.csv') #a file that contains all KNN predictions and LAT/LON coordinates for the observed validation period\n",
    "\n",
    "smooth_gps[smooth_gps['pred'] == 'scent'] #how many scents?\n",
    "\n",
    "#Sometimes, the GPS may not be recording at the exact second the accelerometer predicts a scent mark. \n",
    "#The code below will find the nearest gps pos available and annotate, also recording the time difference to give an idea of location confidence/error \n",
    "#this function finds the nearest datetime in 'items' to the datetime 'pivot'\n",
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))\n",
    "\n",
    "nearest(cut_gps.index, smooth[smooth['pred'] == 'scent'].iloc[0].datetime) #nearest was at 2018-05-11 14:59:47\n",
    "\n",
    "#right = smooth_gps[(smooth_gps.key_0 == nearest(cut_gps.index, smooth[smooth['pred'] == 'right'].iloc[0].datetime))]\n",
    "#left = smooth_gps[(smooth_gps.key_0 == nearest(cut_gps.index, smooth[smooth['pred'] == 'left'].iloc[0].datetime))]\n",
    "scent = smooth_gps[(smooth_gps.key_0 == nearest(cut_gps.index, smooth[smooth['pred'] == 'scent'].iloc[0].datetime))]\n",
    "\n",
    "#Annotate the GPS data with the unobserved predictions too\n",
    "#Load in GPS data files\n",
    "fields = ['Date', ' Time', ' Latitude', ' Longitude']\n",
    "gps = pd.read_csv('Dog_GPS.csv', usecols=fields)\n",
    "gps.columns\n",
    "gps.columns = ['Date', 'Time', 'Latitude', 'Longitude']\n",
    "gps['datetime'] = pd.to_datetime(gps.iloc[:,0] + ' ' + gps.iloc[:,1])\n",
    "gps.set_index('datetime', inplace=True)\n",
    "\n",
    "#cut GPS data to unobserved period\n",
    "start = dog_unobs.datetime.min()\n",
    "end = dog_unobs.datetime.max()\n",
    "unobs_gps_only = gps[(gps.index >= start) & (gps.index <= end)]\n",
    "\n",
    "#merge labels to gps data\n",
    "unobs_gps = dog_unobs.merge(unobs_gps_only, left_on=dog_unobs.datetime, right_on=unobs_gps_only.index, how='left')\n",
    "unobs_gps = unobs_gps.iloc[:,[1,2,5,6]]\n",
    "unobs_gps.set_index('datetime', inplace=True)\n",
    "\n",
    "unobs_gps[(unobs_gps['pred'] == 'scent')] #how many scent-marks?\n",
    "unobs_gps['dt'] = unobs_gps.index\n",
    "#not all scent marks had gps during event, so let's find nearest\n",
    "\n",
    "for i in range(len(scent_marks)):\n",
    "    nrest = nearest(unobs_gps_only.index, unobs_gps[(unobs_gps['pred'] == 'scent')].dt.iloc[i])\n",
    "    scent_marks.iloc[i,4] = pd.to_timedelta(scent_marks.iloc[i].name-nrest, 's').total_seconds()\n",
    "    scent_marks.iloc[i,1] = unobs_gps.Latitude[(unobs_gps.index == nrest)].values\n",
    "    scent_marks.iloc[i,2] = unobs_gps.Longitude[(unobs_gps.index == nrest)].values\n",
    "\n",
    "scent_marks.to_csv('Dog_unobs_scentmarks.csv') #This file describes all of the scent-mark times and locations from the unobserved (deployment) period\n",
    "\n",
    "#This code in case you need all behavioural types\n",
    "#unobs_gps = unobs_gps[unobs_gps.Latitude.notnull()]\n",
    "#unobs_gps.to_csv('gps_unobs_labelled.csv')\n",
    "\n",
    "#Personally, I prefer the mapping tools available in R and QGIS.\n",
    "#If you would prefer to map these locations in Python, here is some simple code that you can extend for your purposes\n",
    "\n",
    "#plot left and right scent marks on a map\n",
    "from gmplot import gmplot\n",
    "smooth_gps.columns\n",
    "\n",
    "#center map\n",
    "gmap = gmplot.GoogleMapPlotter(scent_marks['Latitude'].mean(), scent_marks['Longitude'].mean(), 13) #lat, log, zoom\n",
    "gmap.coloricon = \"http://www.googlemapsmarkers.com/v1/%s/\"\n",
    "#take left and right scent mark coords and plot\n",
    "points = scent_marks\n",
    "gmap.scatter(points.Latitude.values, points.Longitude.values, color='#3B0B39', size=20)\n",
    "gmap.draw('scentmarks.html')\n",
    "\n",
    "#make a map of all locations\n",
    "gps_all = smooth_gps[np.isfinite(smooth_gps.Latitude)]\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(smooth_gps['Latitude'].mean(), smooth_gps['Longitude'].mean(), 13) #lat, log, zoom\n",
    "gmap.coloricon = \"http://www.googlemapsmarkers.com/v1/%s/\"\n",
    "#take left and right scent mark coords and plot\n",
    "gmap.heatmap(gps_all.Latitude.values, gps_all.Longitude.values) #makes a heatmap of point locations\n",
    "gmap.scatter(points.Latitude.values, points.Longitude.values, color='#3B0B39', size=2, marker = False)\n",
    "gmap.draw('allpoints.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tThese steps can then be repeated using data from the unobserved period (obtained during deployment on free living animals for instance) to obtain a record of scent-marking locations. The necessary code segments are presented in the scripts above too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
